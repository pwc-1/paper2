# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
import json
import multiprocessing as mp
import pickle
import os
import mindspore
import numpy as np
import mindspore.nn as nn
import mindspore.ops as ops
from mindspore import Tensor
import ipdb

from src.utils import iou_with_anchors


def load_json(file):
    with open(file) as json_file:
        data = json.load(json_file)
        return data


def load_pickle(path):
    with open(path, 'rb') as f:
        data = pickle.load(f)
    return data


def getDatasetDict(cfg):
    subset = "validation"
    target_fps = cfg.data.frames_fps
    json_data = load_json(cfg.data.video_anno)
    anno_database = json_data['database']
    video_dict = {}
    for video_name, anno in anno_database.items():
        video_subset = anno['subset']
        if video_subset != subset:
            continue
        video_new_info = {}
        video_new_info['duration_second'] = float(anno["duration"])
        video_new_info['annotations'] = anno['annotations']
        video_dict[video_name] = video_new_info
    return video_dict


def soft_nms(df, alpha, t1, t2):
    '''
    df: proposals generated by network;
    alpha: alpha value of Gaussian decaying function;
    t1, t2: threshold for soft nms.
    '''
    df = df.sort_values(by="score", ascending=False)
    # ipdb.set_trace()
    tstart = list(df.xmin3.values[:])
    tend = list(df.xmax3.values[:])
    tscore = list(df.score.values[:])

    rstart = []
    rend = []
    rscore = []

    while len(tscore) > 1 and len(rscore) < 101:
        max_index = tscore.index(max(tscore))
        tmp_iou_list = iou_with_anchors(
            np.array(tstart),
            np.array(tend), tstart[max_index], tend[max_index])
        for idx in range(0, len(tscore)):
            if idx != max_index:
                tmp_iou = tmp_iou_list[idx]
                tmp_width = (tend[max_index] - tstart[max_index])
                if tmp_iou > t1 + (t2 - t1) * tmp_width:
                    tscore[idx] = tscore[idx] * np.exp(-np.power(tmp_iou, 2.5) /
                                                       alpha)

        rstart.append(tstart[max_index])
        rend.append(tend[max_index])
        rscore.append(tscore[max_index])
        tstart.pop(max_index)
        tend.pop(max_index)
        tscore.pop(max_index)

    newDf = pd.DataFrame()
    newDf['score'] = rscore
    newDf['xmin'] = rstart
    newDf['xmax'] = rend
    return newDf


def video_post_process(cfg, video_list, video_dict):
    for video_name in video_list:
        video_info = video_dict[video_name]
        video_duration = video_info["duration_second"]
        # ipdb.set_trace()
        df = pd.read_csv(os.path.join(cfg.eval.output_path, video_name + ".csv"))

        df['score'] = df.ori_score.values[:] * df.preds_iou3.values[:]
        if len(df) > 1:
            snms_alpha = cfg.postprocessing.soft_nms_alpha
            snms_t1 = cfg.postprocessing.soft_nms_low_thres
            snms_t2 = cfg.postprocessing.soft_nms_high_thres
            df = soft_nms(df, snms_alpha, snms_t1, snms_t2)

        df = df.sort_values(by="score", ascending=False)
        proposal_list = []

        for j in range(min(100, len(df))):
            tmp_proposal = {}
            tmp_proposal["score"] = df.score.values[j]
            tmp_proposal["segment"] = [max(0, df.xmin.values[j]) * video_duration,
                                       min(1, df.xmax.values[j]) * video_duration]
            proposal_list.append(tmp_proposal)
        result_dict[video_name] = proposal_list


def post_processing(cfg):
    video_dict = getDatasetDict(cfg)
    video_list = list(video_dict.keys())  # [:100]
    global result_dict
    result_dict = mp.Manager().dict()
    num_videos = len(video_list)

    # post_process = merge_post_process
    post_process = video_post_process
    # post_process(cfg, video_list, video_dict)

    num_videos_per_thread = num_videos // cfg.postprocessing.post_process_thread
    processes = []
    for tid in range(cfg.postprocessing.post_process_thread - 1):
        tmp_video_list = video_list[tid * num_videos_per_thread:(tid + 1) * num_videos_per_thread]
        p = mp.Process(target=post_process, args=(cfg, tmp_video_list, video_dict))
        p.start()
        processes.append(p)
    tmp_video_list = video_list[(cfg.postprocessing.post_process_thread - 1) * num_videos_per_thread:]
    # post_process(cfg, tmp_video_list, video_dict)
    p = mp.Process(target=post_process, args=(cfg, tmp_video_list, video_dict))
    p.start()
    processes.append(p)
    for p in processes:
        p.join()

    result_dict = dict(result_dict)
    output_dict = {"results": result_dict}
    outfile = open(cfg.postprocessing.proposals_result_file, "w")
    json.dump(output_dict, outfile)
    outfile.close()

    save_detection_result(cfg, result_dict)


def save_detection_result(cfg, proposals_dict):
    print("processing detection results^^^^^")
    inference_dataset = "validation"
    classifier_data = load_json(cfg.postprocessing.classifier_result)
    score = classifier_data["results"]
    action = classifier_data["class"]
    detection_result_dict = {}
    use_topk = 2
    topk_weight = [1.0, 1.0, 1.0, 1.0, 1.0]
    max_det_result = 200
    for video_name, proposal_list in proposals_dict.items():
        if video_name not in score:
            print("{} not found!".format(video_name))
            continue
        video_scores = score[video_name]
        softmax = ops.Softmax()
        video_scores = softmax((Tensor(video_scores) * 2.0)).asnumpy().tolist()
        sort_idx = np.argsort(video_scores).tolist()[::-1]
        topk_result_list = []
        for i in range(use_topk):
            label = action[sort_idx[i]]
            #print(video_scores)
            #
            one_score = video_scores[sort_idx[i]]
            for proposal in proposal_list:
                #if proposal["score"] < 0.01:
                #    continue
                det_score = one_score * proposal["score"] * topk_weight[i]
                topk_result_list.append([det_score, label, proposal["segment"]])
        topk_result_list.sort(key=lambda x: x[0], reverse=True)
        result_list = []
        for i in range(min(max_det_result, len(topk_result_list))):
            tmp_detection = {}
            tmp_detection["label"] = topk_result_list[i][1]
            tmp_detection["score"] = topk_result_list[i][0]
            tmp_detection["segment"] = topk_result_list[i][2]
            result_list += [tmp_detection]
        detection_result_dict[video_name] = result_list
        '''
        video_class = action[np.argmax(video_scores)]
        video_score = max(video_scores)
        result_list = []
        for proposal in proposal_list:
            tmp_detection = {}
            tmp_detection["label"] = video_class
            tmp_detection["score"] = video_score * proposal["score"]
            tmp_detection["segment"] = proposal["segment"]
            result_list += [tmp_detection]
        detection_result_dict[video_name] = result_list
        '''
    output_dict = {"results": detection_result_dict}
    with open(cfg.postprocessing.detection_result_file, "w") as f:
        json.dump(output_dict, f)
    print("processing detection results saved!")

